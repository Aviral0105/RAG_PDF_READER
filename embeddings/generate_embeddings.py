import os
import pickle
import numpy as np
import faiss
import fitz  # PyMuPDF for per-page text extraction
from sentence_transformers import SentenceTransformer
from utils.pdf_processing import clean_pdf_text, chunk_text_by_tokens

# âœ… Load embedding model once
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def process_single_pdf(pdf_path):
    """
    Process one PDF: extract, clean, chunk per page, add metadata with page number
    Returns a list of dicts: [{chunk, source, page}]
    """
    pdf_name = os.path.basename(pdf_path)
    print(f"ğŸ“„ Processing: {pdf_name}")

    all_chunks = []

    # âœ… Open PDF with PyMuPDF
    doc = fitz.open(pdf_path)
    total_pages = len(doc)

    for page_num in range(total_pages):
        page = doc.load_page(page_num)
        page_text = page.get_text("text")

        # âœ… Clean text
        cleaned_text = clean_pdf_text(page_text)

        # âœ… Skip empty pages
        if not cleaned_text.strip():
            continue

        # âœ… Chunk text from this page
        chunks = chunk_text_by_tokens(cleaned_text, chunk_size=512, overlap=50)

        # âœ… Attach metadata with PDF name + page number (1-based indexing)
        for c in chunks:
            all_chunks.append({
                "chunk": c,
                "source": pdf_name,
                "page": page_num + 1
            })

    doc.close()
    print(f"âœ… Created {len(all_chunks)} chunks from {pdf_name} ({total_pages} pages)")

    return all_chunks

def process_multiple_pdfs_and_create_index(pdf_folder):
    """
    Process all PDFs in a given folder.
    - Extract & chunk each PDF (page by page)
    - Embed all chunks
    - Save FAISS index & metadata
    """
    # âœ… Check folder
    if not os.path.exists(pdf_folder) or not os.path.isdir(pdf_folder):
        print("âŒ Folder not found!")
        return False

    # âœ… List all PDF files
    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    if not pdf_files:
        print("âš  No PDFs found in this folder.")
        return False

    all_chunks = []

    # âœ… Process each PDF
    for pdf in pdf_files:
        pdf_path = os.path.join(pdf_folder, pdf)
        pdf_chunks = process_single_pdf(pdf_path)
        all_chunks.extend(pdf_chunks)

    print(f"\nâœ… Finished processing {len(pdf_files)} PDFs")
    print(f"âœ… Total chunks created: {len(all_chunks)}")

    if not all_chunks:
        print("âŒ No chunks generated!")
        return False

    # âœ… Extract only chunk texts for embeddings
    chunk_texts = [item["chunk"] for item in all_chunks]

    # âœ… Generate embeddings for all chunks
    print("\nğŸ”„ Generating embeddings for ALL chunks...")
    embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True)
    faiss.normalize_L2(embeddings)

    # âœ… Create FAISS index
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    index.add(np.array(embeddings, dtype="float32"))

    # âœ… Save FAISS index
    faiss.write_index(index, "embeddings/pdf_index.faiss")

    # âœ… Save metadata (chunk + PDF name + page)
    with open("embeddings/chunk_metadata.pkl", "wb") as f:
        pickle.dump(all_chunks, f)

    print("âœ… Multi-PDF index created with page metadata! (FAISS + metadata saved)")
    return True

# # âœ… Allow running Phase 2 standalone
# if _name_ == "_main_":
#     folder = input("ğŸ“‚ Enter folder path containing PDFs: ").strip().strip('"')
#     process_multiple_pdfs_and_create_index(folder)