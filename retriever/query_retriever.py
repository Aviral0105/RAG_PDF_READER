import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

# âœ… File paths
FAISS_PATH = "embeddings/pdf_index.faiss"
METADATA_PATH = "embeddings/chunk_metadata.pkl"

# âœ… Load the same embedding model used in Phase 2
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")


def load_faiss_and_metadata():
    """
    Load the FAISS index (chunk embeddings) and metadata (chunk texts).
    """
    print("ğŸ”„ Loading FAISS index & metadata...")

    # âœ… Load FAISS index created in Phase 2
    index = faiss.read_index(FAISS_PATH)

    # âœ… Load chunk metadata (list of chunk texts)
    with open(METADATA_PATH, "rb") as f:
        metadata = pickle.load(f)

    print(f"âœ… Loaded FAISS index with {index.ntotal} vectors")
    return index, metadata


def retrieve_top_k_chunks(query, faiss_index, chunk_metadata, k=3):
    """
    1ï¸âƒ£ Convert query text â†’ embedding
    2ï¸âƒ£ Normalize query â†’ for cosine similarity
    3ï¸âƒ£ Search FAISS â†’ top-K most similar chunks
    """
    # âœ… 1. Convert query â†’ embedding (384-dim vector)
    query_embedding = embedding_model.encode([query])

    # âœ… 2. Normalize query â†’ dot product = cosine similarity
    faiss.normalize_L2(query_embedding)

    # âœ… 3. Search FAISS for top-K similar chunks
    similarities, indices = faiss_index.search(
        np.array(query_embedding, dtype="float32"), k
    )

    # âœ… 4. Collect top-K chunks with similarity scores
    retrieved_chunks = []
    for idx, sim in zip(indices[0], similarities[0]):
        retrieved_chunks.append((chunk_metadata[idx], sim))

    return retrieved_chunks


if __name__ == "__main__":
    # âœ… Step 1: Load FAISS & metadata
    faiss_index, chunk_metadata = load_faiss_and_metadata()

    while True:
        # âœ… Step 2: Ask user for query
        query = input("\nâ“ Enter your query (or 'exit' to quit): ")
        if query.lower() == "exit":
            break

        # âœ… Step 3: Retrieve top-3 relevant chunks
        results = retrieve_top_k_chunks(
            query,
            faiss_index,
            chunk_metadata,
            k=3  # fixed K = 3
        )

        # âœ… Step 4: Print results
        print("\nğŸ” Top 3 relevant chunks (using cosine similarity):\n")
        for i, (chunk_text, sim) in enumerate(results, 1):
            print(f"--- Result {i} (similarity={sim:.4f}) ---\n{chunk_text[:500]}\n")
